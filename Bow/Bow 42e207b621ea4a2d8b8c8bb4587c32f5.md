# Bow

### Bow(Bag of words)

-단어간 순서와 문법을 무시하고 문서 내의 단어의 개수만을 고려함

-tf 와 idf개념 활용

단점

단어간 순서 판별불가

유사어 판별불가

### TF(Term-Frequency descriptor)

text내의 word의 빈도를 설명해주는 값

순서

구두점제거 - 소문자화 - 토큰화 - 개수 세기

Bow모델에서는 텍스트내의 단어 예를들면 apple이 한번이라도 발생하였는가가 중요한게 아니라

apple이 2번 나타났다 라는 횟수 정보가 중요 —>딕셔너리 내의 key값 중복 허용

```python
import re, string

#구두점 제거
punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))

def strip_punc(corpus):
	return punc_regex.sub('', corpus)

from collections import Counter
import numpy as np
doc = "Apples rule. Apples are the best. Truly, they are. Truly... Truly"
#구두점제거
doc = strip_punc(doc)
#소문자로 변환 후, 공백 기준으로 Counter클래스로 변환
counter = Counter(doc.lower().split())
descriptor = np.array([counter[word] for word in sorted(counter)], dtype=float)
```

counter 변수의 정보는`{'truly': 3, 'apples': 2, 'are': 2, 'rule': 1, 'the': 1, 'best': 1, 'they': 1}`

descriptor 변수의 정보는 정렬된 키값을 기준으로 `[2. 2. 1. 1. 1. 1. 3.]`

### text내의 각 단어의 갯수를 구하는 함수

```python
def to_counter(doc):
	return Counter(strip_punc(doc).lower().split())
```

### vocabulary(단어 모음집)

텍스트내의 모든 단어를 사전과 같이 통합하여 만든 TF-descriptor

ex)

doc1 = “i am a dog”

doc2 = “i am a cat”

doc1의 tf —> array([1, 1, 1, 1])

doc2의 tf —> array([1, 1, 1, 1])

위처럼 다른 문장이지만 같은 tf_descriptor

따라서 텍스트내의 모든 단어가 들어있는 통합 vocabulary 필요

```python
#count를 제외한 단어의 리스트만 생성
#매개변수는 Iterable[Counter], 여러 counter들에 대한 iterable, 리스트 형태로 넣어줘야함
def to_vocab(counters):
	vocab = set()
	for counter in counters:
		#counters의 각 counter의 키를 vocab에 업데이트, set형이므로 중복 안됨
		vocab.update(counter.keys())
	return sorted(vocab) #알파벳 순으로 정렬
```

### vocabulary를 바탕으로 알고싶은 text를 입력하여 빈도만 출력해주는 함수

```python
def to_tf(counter, vocab):
	lst = [] 
	for word in vocab:
		#counter[word] 는 word에 상응하는 key를 출력하는 것을 알아야됨;
		lst.append(counter[word])
	return np.array(lst. dtype = float)
```

### 활용 - 문장4개의 tf를 행렬로 나타내보기

```python
doc_1 = "I am a dog."
doc_2 = "I am a cat!"
doc_3 = "I am not a dog"
doc_4 = "I am not a cat, am I!?!"

counters = []
for doc in [doc_1, doc_2, doc_3, doc_4]:
	counter = to_counter(doc1)
	counters.append(counter)

#vocabulary(단어 모음집) 생성
bag = to_vocab(counters)

ret = []
for counter in counters:
	ret.append(to_tf(counter, bag))
#vstack은 한줄로 늘어져있는 배열을 세로로 결합
return np.vstack(ret)

#결과값
[[1. 1. 0. 1. 1. 0.]
 [1. 1. 1. 0. 1. 0.]
 [1. 1. 0. 1. 1. 1.]
 [1. 2. 1. 0. 2. 1.]]
```

### 활용 - 최대 빈출 단어 갯수를 설정하여 출력

```python
def to_vocab(counters, k = None): #k가 빈도순으로 출력될 최대 단어의 갯수
	vocab = Counter()
	for counter in counters:
		vocab.update(counter)
	return sorted(i for i,j in vocab.most_common(k))
```

위의 리턴에서 볼 수 있듯이 Counter클래스는 절대keys, values로 접근 x 

무조건 인덱스로 접근

### 최대 빈출 단어의 경향성

최대 빈출 단어는 주로 불용어이다(a, after, by, but, to, at)

### 활용 - 불용어를 제외한 최대 빈출 단어 출력

```python
#counter - iterable, stop_words = 단어의 모음,리스트도 상관x
def to_vocab(counters, K = None, stop_words = tuple())
	vocab = Counter()
	for counter in counters:
		vocab.update(counter)
	for word in set(stop_words):
		vocab.pop(word,None) #단어가 없으면 return None
	
	collections = []
	for i, j in vocab.most_common(k):
		collections.append(i)
	return sorted(collections)
```

### Bow의 문제점

여기까지 살펴본 바로 Bow 모델은 단어간 순서를 완전히 무시한다

단순한 만큼 문장을 거칠게 분석

### tf함수를 count가 아닌 frequency로 출력해보기

```python
def to_tf(counter, vocab):
	lst = []
	for word in vocab:
		lst.append(counter[word])
	ret = np.array(lst, dtype = float)
	return ret / sum(ret)
```

## IDF (Inverse Document Frequency)

IDF = log(N / n) -N:전체 문서 총개수, n: 용어 t가 등장하는 문서의총갯수

```python
#vocab은 우리가 관심을 두는 단어의 리스트
def to_idf(vocab, counters):
	N = len(counters)
	lst = []
	
	for t in vocab:
		cnt = 0
		for counter in counters:
			if( t in counter): cnt += 1
		lst.append(cnt)
	df = np.array(lst, dtype = float)
	return np.log10(N / df)
```